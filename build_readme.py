from python_graphql_client import GraphqlClient
import feedparser
import httpx
import json
import pathlib
import re
import os
import datetime

# ç¡®å®šä»“åº“çš„æ ¹ç›®å½•
root = pathlib.Path(__file__).parent.resolve()
client = GraphqlClient(endpoint="https://api.github.com/graphql")

# ä»ç¯å¢ƒå˜é‡ä¸­è·å– TOKEN
TOKEN = os.environ.get("GH_TOKEN", "")

def replace_chunk(content, marker, chunk, inline=False):
    """
    æ›¿æ¢ Markdown/æ–‡æœ¬ä¸­çš„ç‰¹å®šæ ‡è®°å—ã€‚
    """
    r = re.compile(
        r"<!\-\- {} starts \-\->.*<!\-\- {} ends \-\->".format(marker, marker),
        re.DOTALL,
    )
    if not inline:
        chunk = "\n{}\n".format(chunk)
    chunk = "<!-- {} starts -->{}<!-- {} ends -->".format(marker, chunk, marker)
    return r.sub(chunk, content)

def formatGMTime(timestamp):
    """
    æ ¼å¼åŒ– GMT æ—¶é—´æˆ³ï¼Œå¹¶è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´ (UTC+8)ã€‚
    """
    GMT_FORMAT = '%a, %d %b %Y %H:%M:%S GMT'
    try:
        dateStr = datetime.datetime.strptime(timestamp, GMT_FORMAT) + datetime.timedelta(hours=8)
        return dateStr.date()
    except ValueError:
        return datetime.date.today()

def make_query(after_cursor=None):
    """
    æ„å»º GraphQL æŸ¥è¯¢ï¼Œç”¨äºåˆ†é¡µè·å–ä»“åº“ Releases ä¿¡æ¯ã€‚
    """
    return """
query {
  viewer {
    repositories(first: 100, privacy: PUBLIC, after:AFTER) {
      pageInfo {
        hasNextPage
        endCursor
      }
      nodes {
        name
        description
        url
        releases(last:1) {
          totalCount
          nodes {
            name
            publishedAt
            url
          }
        }
      }
    }
  }
}
""".replace(
        "AFTER", '"{}"'.format(after_cursor) if after_cursor else "null"
    )

def fetch_releases(oauth_token):
    """
    é€šè¿‡ GraphQL API è·å–æ‰€æœ‰ä»“åº“çš„æœ€æ–° Releaseã€‚
    """
    releases = []
    repo_names = set()
    has_next_page = True
    after_cursor = None

    # å¦‚æœ TOKEN ä¸å­˜åœ¨ï¼Œåˆ™è¿”å›ç©ºåˆ—è¡¨å¹¶æ‰“å°è­¦å‘Š
    if not oauth_token:
        print("Warning: GH_TOKEN is missing or empty. Skipping release fetch.")
        return []

    while has_next_page:
        try:
            # è¿™é‡Œçš„ Authorization ä½¿ç”¨äº†æ‚¨çš„ PAT/Secret
            data = client.execute(
                query=make_query(after_cursor),
                headers={"Authorization": "Bearer {}".format(oauth_token)},
            )
        except Exception as e:
            # æ•è· HTTP æˆ–è¿æ¥é”™è¯¯
            print(f"Error fetching releases from GraphQL: {e}")
            return []

        # æ£€æŸ¥æ˜¯å¦æœ‰ API çº§åˆ«çš„é”™è¯¯ï¼ˆä¾‹å¦‚ 401 Unauthorizedï¼‰
        if "errors" in data:
            print("GraphQL API returned errors:")
            print(json.dumps(data["errors"], indent=4))
            return []

        repositories = data.get("data", {}).get("viewer", {}).get("repositories", {})
        if not repositories:
            break
            
        for repo in repositories.get("nodes", []):
            # æ£€æŸ¥æ˜¯å¦æœ‰ release ä¸”ä»“åº“åæœªé‡å¤
            if repo["releases"]["totalCount"] and repo["name"] not in repo_names:
                repo_names.add(repo["name"])
                if repo["releases"]["nodes"]:
                    release_node = repo["releases"]["nodes"][0]
                    releases.append(
                        {
                            "repo": repo["name"],
                            "repo_url": repo["url"],
                            "description": repo["description"],
                            "release": release_node["name"]
                            .replace(repo["name"], "")
                            .strip(),
                            "published_at": release_node["publishedAt"].split("T")[0],
                            "url": release_node["url"],
                        }
                    )
        
        page_info = repositories.get("pageInfo", {})
        has_next_page = page_info.get("hasNextPage", False)
        after_cursor = page_info.get("endCursor")
        
    return releases

# --- å¤–éƒ¨æ•°æ®æŠ“å–å‡½æ•° ---
def fetch_code_time():
    """è·å– Gist ä¸­çš„ä»£ç æ—¶é—´ç»Ÿè®¡ä¿¡æ¯"""
    try:
        return httpx.get(
            "https://gist.githubusercontent.com/pseudoyu/48675a7b5e3cca534e7817595d566003/raw/"
        )
    except Exception as e:
        print(f"Error fetching code time: {e}")
        return type('obj', (object,), {'text' : 'Failed to load code time.'})() # è¿”å›ä¸€ä¸ªæ¨¡æ‹Ÿå¯¹è±¡

def fetch_douban():
    """è·å–è±†ç“£ RSS è®¢é˜…æ¡ç›®"""
    try:
        entries = feedparser.parse("https://www.douban.com/feed/people/pseudo-yu/interests")["entries"]
        return [
            {
                "title": item["title"],
                "url": item["link"].split("#")[0],
                "published": formatGMTime(item["published"])
            }
            for item in entries
        ]
    except Exception as e:
        print(f"Error fetching douban feed: {e}")
        return []

def fetch_blog_entries():
    """è·å–åšå®¢ RSS è®¢é˜…æ¡ç›®"""
    try:
        entries = feedparser.parse("https://www.pseudoyu.com/zh/index.xml")["entries"]
        return [
            {
                "title": entry["title"],
                "url": entry["link"].split("#")[0],
                "published": entry["published"].split("T")[0],
            }
            for entry in entries
        ]
    except Exception as e:
        print(f"Error fetching blog feed: {e}")
        return []

# --- ä¸»é€»è¾‘ ---
if __name__ == "__main__":
    readme = root / "README.md"
    project_releases = root / "releases.md"
    
    # ğŸ“Œ ä¿®å¤æ–‡ä»¶ç¼ºå¤±é”™è¯¯: æ£€æŸ¥å¹¶åˆ›å»º releases.md
    if not project_releases.exists():
        initial_content = """
# Project Releases

This file is automatically generated by GitHub Actions.

<!-- release_count starts -->0<!-- release_count ends --> releases tracked.

<!-- recent_releases starts -->
No releases found yet.
<!-- recent_releases ends -->
"""
        project_releases.open("w", encoding="utf-8").write(initial_content.strip())
        print("Created initial releases.md file.")

    # 1. è·å–å¹¶å¤„ç† Releases
    releases = fetch_releases(TOKEN)
    releases.sort(key=lambda r: r["published_at"], reverse=True)
    
    # README.md æ›´æ–°
    md = "\n".join(
        [
            "* <a href={url} target='_blank'>{repo} {release}</a> - {published_at}".format(**release)
            for release in releases[:10]
        ]
    )
    readme_contents = readme.open().read()
    rewritten = replace_chunk(readme_contents, "recent_releases", md)

    # releases.md æ›´æ–°
    project_releases_md = "\n".join(
        [
            (
                "* **[{repo}]({repo_url})**: [{release}]({url}) - {published_at}\n"
                "<br>{description}"
            ).format(**release)
            for release in releases
        ]
    )
    # ç¡®ä¿æ–‡ä»¶å­˜åœ¨åæ‰è¯»å–
    project_releases_content = project_releases.open().read()
    project_releases_content = replace_chunk(
        project_releases_content, "recent_releases", project_releases_md
    )
    project_releases_content = replace_chunk(
        project_releases_content, "release_count", str(len(releases)), inline=True
    )
    project_releases.open("w").write(project_releases_content)

    # 2. æ›´æ–° Code Time
    code_time_text = "\n```text\n"+fetch_code_time().text+"\n```\n"
    rewritten = replace_chunk(rewritten, "code_time", code_time_text)

    # 3. æ›´æ–° Douban
    doubans = fetch_douban()[:5]
    doubans_md = "\n".join(
        ["* <a href='{url}' target='_blank'>{title}</a> - {published}".format(**item) for item in doubans]
    )
    rewritten = replace_chunk(rewritten, "douban", doubans_md)

    # 4. æ›´æ–° Blog Entries
    entries = fetch_blog_entries()[:6]
    entries_md = "\n".join(
        ["* <a href={url} target='_blank'>{title}</a>".format(**entry) for entry in entries]
    )
    rewritten = replace_chunk(rewritten, "blog", entries_md)

    # å†™å…¥æœ€ç»ˆçš„ README.md
    readme.open("w").write(rewritten)